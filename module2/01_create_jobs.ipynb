{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": "# Module 2: Creating CML Jobs Programmatically\n\n## Overview\n\nIn Module 1, you built a complete ML pipeline using Python scripts executed manually. In Module 2, we'll automate this pipeline using **CML Jobs** - scheduled or triggered tasks that run your scripts at scale.\n\nThis notebook teaches you how to **create jobs programmatically** using the CML API. Instead of clicking through the UI, you'll write code to define, configure, and manage jobs. This approach enables:\n\n- **Automation**: Create multiple jobs with consistent configurations\n- **Reproducibility**: Version control your job definitions\n- **Integration**: Orchestrate complex workflows programmatically\n- **Scalability**: Deploy jobs across projects and environments\n- **GitOps**: Store job definitions in version control\n\n---\n\n## Learning Goals\n\nBy the end of this notebook, you'll understand:\n\n1. ✅ How to authenticate with CML API\n2. ✅ How to query available ML runtimes\n3. ✅ How to retrieve project metadata\n4. ✅ How to define a job using `cmlapi.CreateJobRequest`\n5. ✅ How to create jobs programmatically\n6. ✅ How to configure job dependencies\n\n**Note**: We will **create** jobs but not **run** them in this notebook. Running jobs will be covered separately.\n\n## Module 2 Job Pipeline Structure\n\nThis notebook creates **2 jobs** that work together in a monitoring pipeline:\n\n```\nJob 1: Prepare Artificial Data\n    ↓\n    └─→ Creates training dataset\n\nJob 2: Monitor Pipeline\n    ├─ Period 0: Get predictions → Load ground truth → Check model\n    ├─ Period 1: Get predictions → Load ground truth → Check model\n    ├─ Period 2: Get predictions → Load ground truth → Check model\n    └─ ... (repeats for all periods)\n```\n\nKey difference from previous design:\n- **Old**: 4 jobs with manual state passing and job chaining\n- **New**: 2 jobs with self-contained period management in Job 2\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-1",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Authentication\n",
    "\n",
    "### Step 1.1: Import Required Libraries\n",
    "\n",
    "We need key libraries:\n",
    "- **`cmlapi`**: Official CML Python API client for programmatic access\n",
    "- **`os`**: To access environment variables set by CML\n",
    "- **`json`**: For parsing runtime filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cmlapi\n",
    "import json\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-1-2",
   "metadata": {},
   "source": [
    "### Step 1.2: Create CML API Client\n",
    "\n",
    "The CML API client authenticates your requests using credentials automatically provided by CML:\n",
    "\n",
    "- **`CDSW_API_URL`**: The CML API endpoint for your workspace\n",
    "- **`CDSW_APIV2_KEY`**: Your authentication token\n",
    "\n",
    "We remove `/api/v1` from the URL since the client handles API versioning internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-auth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CML API client created successfully\n",
      "  API URL: https://ml-dbfc64d1-783.go01-dem.ylcu-atmi.cloudera.site/api/v1\n"
     ]
    }
   ],
   "source": [
    "# Create the CML API client\n",
    "# The client uses environment variables automatically set by CML for authentication\n",
    "client = cmlapi.default_client(\n",
    "    url=os.getenv(\"CDSW_API_URL\").replace(\"/api/v1\", \"\"),\n",
    "    cml_api_key=os.getenv(\"CDSW_APIV2_KEY\")\n",
    ")\n",
    "\n",
    "print(\"✓ CML API client created successfully\")\n",
    "print(f\"  API URL: {os.getenv('CDSW_API_URL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-1-3",
   "metadata": {},
   "source": [
    "### Step 1.3: Retrieve Available ML Runtimes\n",
    "\n",
    "Jobs execute in **ML Runtimes** - pre-configured Docker containers with specific Python versions, libraries, and GPU support. We need to query available runtimes that match our requirements.\n",
    "\n",
    "In this example, we filter for:\n",
    "- **kernel**: \"Python 3.10\" - Python version\n",
    "- **edition**: \"Standard\" - Standard edition\n",
    "- **editor**: \"PBJ Workbench\" - JupyterLab interface for development\n",
    "\n",
    "**Important Notes:**\n",
    "- Runtimes from **2024.05 onwards** include ML packages (pandas, numpy, scikit-learn, etc.)\n",
    "- We select the **NEWEST runtime** to ensure all dependencies are included\n",
    "- Older runtimes (2023.x) lack pandas and will cause `ModuleNotFoundError` in jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-runtimes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 10 available runtime(s)\n",
      "\n",
      "Runtime 1:\n",
      "  Version: 2023.05.1-b4\n",
      "  Kernel: Python 3.10\n",
      "  Edition: Standard\n",
      "  Image: container.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2023.05.1-b4\n",
      "\n",
      "Runtime 2:\n",
      "  Version: 2023.05.1-b4\n",
      "  Kernel: Python 3.10\n",
      "  Edition: Standard\n",
      "  Image: docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2023.05.1-b4\n",
      "\n",
      "Runtime 3:\n",
      "  Version: 2023.05.2-b7\n",
      "  Kernel: Python 3.10\n",
      "  Edition: Standard\n",
      "  Image: docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2023.05.2-b7\n",
      "\n",
      "Runtime 4:\n",
      "  Version: 2023.08.1-b6\n",
      "  Kernel: Python 3.10\n",
      "  Edition: Standard\n",
      "  Image: docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2023.08.1-b6\n",
      "\n",
      "Runtime 5:\n",
      "  Version: 2023.08.2-b8\n",
      "  Kernel: Python 3.10\n",
      "  Edition: Standard\n",
      "  Image: docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2023.08.2-b8\n",
      "\n",
      "Runtime 6:\n",
      "  Version: 2023.12.1-b8\n",
      "  Kernel: Python 3.10\n",
      "  Edition: Standard\n",
      "  Image: docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2023.12.1-b8\n",
      "\n",
      "Runtime 7:\n",
      "  Version: 2024.02.1-b4\n",
      "  Kernel: Python 3.10\n",
      "  Edition: Standard\n",
      "  Image: docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2024.02.1-b4\n",
      "\n",
      "Runtime 8:\n",
      "  Version: 2024.05.1-b8\n",
      "  Kernel: Python 3.10\n",
      "  Edition: Standard\n",
      "  Image: docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2024.05.1-b8\n",
      "\n",
      "Runtime 9:\n",
      "  Version: 2024.05.2-b14\n",
      "  Kernel: Python 3.10\n",
      "  Edition: Standard\n",
      "  Image: docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2024.05.2-b14\n",
      "\n",
      "Runtime 10:\n",
      "  Version: 2024.10.1-b12\n",
      "  Kernel: Python 3.10\n",
      "  Edition: Standard\n",
      "  Image: docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2024.10.1-b12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query available runtimes matching our criteria\n",
    "# The search_filter is a JSON string with our runtime requirements\n",
    "available_runtimes = client.list_runtimes(\n",
    "    search_filter=json.dumps({\n",
    "        \"kernel\": \"Python 3.10\",\n",
    "        \"edition\": \"Standard\",\n",
    "        \"editor\": \"PBJ Workbench\"\n",
    "    })\n",
    ")\n",
    "print(f\"✓ Found {len(available_runtimes.runtimes)} available runtime(s)\\n\")\n",
    "\n",
    "# Display all available runtimes\n",
    "for i, runtime in enumerate(available_runtimes.runtimes, 1):\n",
    "    print(f\"Runtime {i}:\")\n",
    "    print(f\"  Version: {runtime.full_version}\")\n",
    "    print(f\"  Kernel: {runtime.kernel}\")\n",
    "    print(f\"  Edition: {runtime.edition}\")\n",
    "    print(f\"  Image: {runtime.image_identifier}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-1-4",
   "metadata": {},
   "source": [
    "### Step 1.4: Select and Store the Latest Runtime\n",
    "\n",
    "We'll use the **first (most recent) runtime** from our filtered list. This runtime identifier will be passed to job creation calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-runtime-select",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Selected Runtime (NEWEST):\n",
      "  Version: 2024.10.1-b12\n",
      "  Image: docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-pbj-workbench-python3.10-standard:2024.10.1-b12\n",
      "  Note: Only 2024+ runtimes include ML packages (pandas, numpy, scikit-learn)\n",
      "  ✓ Runtime includes pandas and ML libraries\n"
     ]
    }
   ],
   "source": [
    "# Select the latest runtime (sort by version, newest first)\n",
    "sorted_runtimes = sorted(available_runtimes.runtimes, key=lambda r: r.full_version, reverse=True)\n",
    "JOB_IMAGE_ML_RUNTIME = sorted_runtimes[0].image_identifier\n",
    "\n",
    "print(f\"✓ Selected Runtime (NEWEST):\")\n",
    "print(f\"  Version: {sorted_runtimes[0].full_version}\")\n",
    "print(f\"  Image: {JOB_IMAGE_ML_RUNTIME}\")\n",
    "print(f\"  Note: Only 2024+ runtimes include ML packages (pandas, numpy, scikit-learn)\")\n",
    "\n",
    "# Verify this is a recent runtime with pandas\n",
    "if \"2024\" not in sorted_runtimes[0].full_version:\n",
    "    print(f\"\\n⚠️  WARNING: Selected runtime {sorted_runtimes[0].full_version} may not have pandas!\")\n",
    "    print(f\"  Recommended: Use 2024.05+ or 2024.10+\")\n",
    "else:\n",
    "    print(f\"  ✓ Runtime includes pandas and ML libraries\")\n",
    "\n",
    "# Store in environment variable for use in job definitions\n",
    "os.environ['JOB_IMAGE_ML_RUNTIME'] = JOB_IMAGE_ML_RUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-1-5",
   "metadata": {},
   "source": [
    "### Step 1.5: Retrieve Project Metadata\n",
    "\n",
    "Jobs are created within a specific CML project. We retrieve the current project's metadata to get its ID and other information needed for job creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-project",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project Retrieved:\n",
      "  Project ID: crfy-i66p-le3j-cdss\n",
      "  Project Name: CAI Baseline MLOPS\n",
      "  Description: \n"
     ]
    }
   ],
   "source": [
    "# Get metadata for the current CML project\n",
    "# CDSW_PROJECT_ID is automatically set when you run code in a CML project\n",
    "project = client.get_project(\n",
    "    project_id=os.getenv(\"CDSW_PROJECT_ID\")\n",
    ")\n",
    "\n",
    "print(f\"✓ Project Retrieved:\")\n",
    "print(f\"  Project ID: {project.id}\")\n",
    "print(f\"  Project Name: {project.name}\")\n",
    "print(f\"  Description: {project.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-2",
   "metadata": {},
   "source": "---\n\n## Section 2: Job Creation - The Monitoring Pipeline\n\n### Understanding Job Definitions\n\nA CML job is defined using `cmlapi.CreateJobRequest` with these key parameters:\n\n| Parameter | Purpose | Example |\n|-----------|---------|----------|\n| `project_id` | Which project to create the job in | `project.id` |\n| `name` | Human-readable job name | `\"Monitor Pipeline\"` |\n| `script` | Path to Python script to run | `\"03_monitoring_pipeline.py\"` |\n| `cpu` | CPU cores to allocate | `2` |\n| `memory` | RAM in GB to allocate | `4` |\n| `runtime_identifier` | ML Runtime image to use | `JOB_IMAGE_ML_RUNTIME` |\n| `environment` | Environment variables for the job | `{\"BATCH_SIZE\": \"250\"}` |\n\n### Step 2.1: Create Job 1 - Prepare Artificial Data\n\nThis is the entry point to the monitoring pipeline. It prepares synthetic data for the workflow."
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-2-1",
   "metadata": {},
   "source": [
    "### Step 2.1: Create Job 02 - Prepare Artificial Data\n",
    "\n",
    "This is the entry point to the monitoring pipeline. It prepares synthetic data for the workflow.\n",
    "The notebook will then trigger Job 03.1 (Get Predictions) after completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-job02-define",
   "metadata": {},
   "outputs": [],
   "source": "# Define Job 1: Prepare Artificial Data\n# This is the entry point that creates the synthetic dataset\n\njob_body_prepare_data = cmlapi.CreateJobRequest(\n    project_id=project.id,\n    name=\"Mod 2 Job 1: Prepare Artificial Data\",\n    script=\"module2/02_prepare_artificial_data.py\",\n    cpu=1,\n    memory=2,\n    runtime_identifier=os.getenv('JOB_IMAGE_ML_RUNTIME'),\n)\n\nprint(\"Job 1 Definition - Prepare Artificial Data:\")\nprint(f\"  Script: {job_body_prepare_data.script}\")\nprint(f\"  CPU: {job_body_prepare_data.cpu} cores\")\nprint(f\"  Memory: {job_body_prepare_data.memory} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-job02-create",
   "metadata": {},
   "outputs": [],
   "source": "# Create Job 1 in CML\n# This registers the job with CML but does NOT run it\njob_1_prepare_data = client.create_job(\n    body=job_body_prepare_data,\n    project_id=str(project.id)\n)\n\nprint(\"\\n✓ Job 1 Created Successfully!\")\nprint(f\"  Job ID: {job_1_prepare_data.id}\")\nprint(f\"  Job Name: {job_1_prepare_data.name}\")\nprint(f\"  Script: {job_1_prepare_data.script}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-2-2",
   "metadata": {},
   "source": "## Section 2: Job Creation - The Monitoring Pipeline\n\nNow we'll create the 2 CML jobs that power the monitoring system."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-job031-define",
   "metadata": {},
   "outputs": [],
   "source": "# Define Job 2: Monitor Pipeline\n# This is the main monitoring job that processes all periods sequentially\n\njob_body_monitor_pipeline = cmlapi.CreateJobRequest(\n    project_id=project.id,\n    name=\"Mod 2 Job 2: Monitor Pipeline\",\n    script=\"module2/03_monitoring_pipeline.py\",\n    cpu=2,\n    memory=4,\n    runtime_identifier=os.getenv('JOB_IMAGE_ML_RUNTIME'),\n)\n\nprint(\"Job 2 Definition - Monitor Pipeline:\")\nprint(f\"  Script: {job_body_monitor_pipeline.script}\")\nprint(f\"  CPU: {job_body_monitor_pipeline.cpu} cores\")\nprint(f\"  Memory: {job_body_monitor_pipeline.memory} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-job031-create",
   "metadata": {},
   "outputs": [],
   "source": "# Create Job 2 in CML\njob_2_monitor_pipeline = client.create_job(\n    body=job_body_monitor_pipeline,\n    project_id=str(project.id)\n)\n\nprint(\"\\n✓ Job 2 Created Successfully!\")\nprint(f\"  Job ID: {job_2_monitor_pipeline.id}\")\nprint(f\"  Job Name: {job_2_monitor_pipeline.name}\")\nprint(f\"  Script: {job_2_monitor_pipeline.script}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-2-3",
   "metadata": {},
   "source": "### Step 2.2: Create Job 2 - Monitor Pipeline\n\nThis job processes all periods sequentially in a single execution.\nIt automatically handles period state management and degradation detection."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-job032-define",
   "metadata": {},
   "outputs": [],
   "source": "# Note: Jobs 3 and 4 from the original design have been consolidated into Job 2\n# The Monitor Pipeline job (03_monitoring_pipeline.py) handles:\n# - Getting predictions (original Job 3.1)\n# - Loading ground truth (original Job 3.2)  \n# - Checking model (original Job 3.3)\n# All in a single, self-contained execution per run\n\nprint(\"✓ Job 1 and Job 2 have been created successfully!\")\nprint(\"  The previous 4-job design has been consolidated into 2 jobs\")\nprint(\"  Jobs 3, 4, and 5 are no longer needed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-job032-create",
   "metadata": {},
   "outputs": [],
   "source": "# This cell is intentionally empty\n# Jobs 3 and 4 from the original design are no longer created separately\npass"
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-2-4",
   "metadata": {},
   "source": [
    "### Step 2.4: Create Job 03.3 - Check Model\n",
    "\n",
    "This job validates model accuracy for the current period and orchestrates the monitoring pipeline.\n",
    "It is triggered by Job 03.2 after ground truth labels are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-job033-define",
   "metadata": {},
   "outputs": [],
   "source": "# This cell is intentionally empty\npass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-job033-create",
   "metadata": {},
   "outputs": [],
   "source": "# This cell is intentionally empty\npass"
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Job Orchestration and Summary\n",
    "\n",
    "### Understanding the Job Pipeline\n",
    "\n",
    "The four jobs we created work together in an automated monitoring pipeline:\n",
    "\n",
    "**Pipeline Flow:**\n",
    "\n",
    "```\n",
    "Job 02: Prepare Artificial Data\n",
    "   (Entry point - prepares synthetic data)\n",
    "        ↓\n",
    "   job_02_prepare_data.py calls:\n",
    "   client.create_job_run(..., job_id=job_031_get_predictions.id)\n",
    "        ↓\n",
    "Job 03.1: Get Predictions\n",
    "   (Processes predictions for current period in batches)\n",
    "        ↓\n",
    "   job_031_get_predictions.py calls:\n",
    "   client.create_job_run(..., job_id=job_032_load_ground_truth.id)\n",
    "        ↓\n",
    "Job 03.2: Load Ground Truth\n",
    "   (Loads and processes ground truth labels)\n",
    "        ↓\n",
    "   job_032_load_ground_truth.py calls:\n",
    "   client.create_job_run(..., job_id=job_033_check_model.id)\n",
    "        ↓\n",
    "Job 03.3: Check Model\n",
    "   (Validates accuracy and detects degradation)\n",
    "        ↓\n",
    "   Decides: Continue pipeline or stop\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. Job 02 runs and prepares the data\n",
    "2. At the end, Job 02's script calls `client.create_job_run()` to trigger Job 03.1\n",
    "3. Job 03.1 gets predictions for the period\n",
    "4. At the end, Job 03.1's script calls `client.create_job_run()` to trigger Job 03.2\n",
    "5. Job 03.2 loads ground truth labels\n",
    "6. At the end, Job 03.2's script calls `client.create_job_run()` to trigger Job 03.3\n",
    "7. Job 03.3 validates model accuracy and decides next steps\n",
    "\n",
    "This approach allows:\n",
    "- ✅ Sequential execution (each job waits for the previous to complete)\n",
    "- ✅ Data passing between jobs (via files or environment variables)\n",
    "- ✅ Dynamic decision logic (Job 03.3 can decide whether to continue the pipeline)\n",
    "- ✅ Resilience (if a job fails, the pipeline stops safely)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-section-3-1",
   "metadata": {},
   "source": [
    "### Step 3.1: Display Complete Job Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"ALL 2 JOBS CREATED SUCCESSFULLY\")\nprint(\"=\"*80)\n\njobs = [\n    (\"Job 1\", job_1_prepare_data, \"Entry point - prepares synthetic data\"),\n    (\"Job 2\", job_2_monitor_pipeline, \"Integrated monitoring - processes all periods\"),\n]\n\nfor label, job, description in jobs:\n    print(f\"\\n{label}: {job.name}\")\n    print(f\"  ID: {job.id}\")\n    print(f\"  Script: {job.script}\")\n    print(f\"  CPU: {job.cpu} cores\")\n    print(f\"  Memory: {job.memory} GB\")\n    print(f\"  Purpose: {description}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"NEXT STEPS\")\nprint(\"=\"*80)\nprint(\"\"\"\n✓ All 2 jobs have been created programmatically\n✓ Jobs are now registered in CML but have NOT been run\n\nTo run the jobs:\n\n1. Option A - Run via CML UI:\n   - Go to your project's Jobs tab\n   - Click on \"Mod 2 Job 1: Prepare Artificial Data\"\n   - Select \"Run Now\"\n   - Wait for completion\n   \n   - Then click on \"Mod 2 Job 2: Monitor Pipeline\"\n   - Select \"Run Now\"\n   - The job will process all periods sequentially\n   - Check results in data/monitoring_results.json\n\n2. Option B - Run via API (next notebook):\n   - Use client.create_job_run() to trigger Job 1\n   - Wait for completion\n   - Use client.create_job_run() to trigger Job 2\n   - Poll for completion status\n\nKey Advantages of This Design:\n   • Simpler: Only 2 jobs instead of 4\n   • Reliable: No job chaining failures or state passing issues\n   • Faster: No overhead from starting multiple jobs\n   • Cleaner logs: All output in one execution\n   • Self-contained: No external state file dependencies\n   • Flexible: Can run specific period ranges via command-line args\n\nKey Learning Points:\n   • Jobs are created once, run many times\n   • Each run can have different environment variables or arguments\n   • Job names help identify jobs programmatically\n   • cmlapi enables GitOps workflows for job management\n   • This 2-job pipeline automates the complete monitoring workflow\n   • Period state is managed internally, not via external files\n\"\"\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell-key-concepts",
   "metadata": {},
   "source": "---\n\n## Key Concepts Review\n\n### Authentication\n```python\nclient = cmlapi.default_client(\n    url=os.getenv(\"CDSW_API_URL\").replace(\"/api/v1\", \"\"),\n    cml_api_key=os.getenv(\"CDSW_APIV2_KEY\")\n)\n```\nCML automatically provides API credentials in environment variables.\n\n### Selecting Runtimes\n```python\navailable_runtimes = client.list_runtimes(\n    search_filter=json.dumps({\"kernel\": \"Python 3.10\"})\n)\n```\nQuery available ML Runtimes with specific configurations (Python version, GPU, editor).\n\n### Creating Jobs\n```python\njob_body = cmlapi.CreateJobRequest(\n    project_id=project.id,\n    name=\"Job Name\",\n    script=\"script.py\",\n    cpu=2,\n    memory=4,\n    runtime_identifier=runtime_id,\n    environment={\"VAR\": \"value\"}\n)\n\njob = client.create_job(body=job_body, project_id=project.id)\n```\nDefine job configuration and create it in CML (does NOT run automatically).\n\n### Internal State Management (New Design)\nInstead of passing state between jobs via environment variables and job chaining:\n```python\n# OLD (4 jobs): Job1 → triggers Job2 → triggers Job3 → triggers Job4\n# NEW (2 jobs): Job1 → Job2 processes all periods internally\n\n# Job 2 (Monitor Pipeline) manages period state:\nfor period in range(START_PERIOD, END_PERIOD + 1):\n    # Phase 1: Get Predictions\n    # Phase 2: Load Ground Truth\n    # Phase 3: Check Model\n    # Decision: Continue or exit\n```\nPeriod state is managed in-memory within a single job execution.\n\n---\n\n## Summary: The 2-Job Pipeline\n\nYou've just created a simplified, production-grade monitoring pipeline using CML Jobs:\n\n| Job | Purpose | Runs |\n|-----|---------|------|\n| 1 | Prepare synthetic data | Once (one-time setup) |\n| 2 | Monitor all periods | Once (processes all periods sequentially) |\n\nThis is simpler and more reliable than the original 4-job design while maintaining all the same functionality.\n\n### Design Improvements\n\n**Original Design (4 Jobs)**:\n```\nJob 1: Prepare Data\n   ↓ triggers\nJob 2: Get Predictions (Period 0)\n   ↓ triggers\nJob 3: Load Ground Truth (Period 0)\n   ↓ triggers\nJob 4: Check Model (Period 0)\n   ↓ triggers\nJob 2: Get Predictions (Period 1)\n   ↓ ... (repeats for each period)\n```\nProblems: State passing, job chaining failures, multiple job executions, complexity\n\n**New Design (2 Jobs)**:\n```\nJob 1: Prepare Data (one-time)\n   ↓ manual trigger\nJob 2: Monitor Pipeline\n   ├─ for period in [0, 1, 2, ...]:\n   │   ├─ Get Predictions\n   │   ├─ Load Ground Truth\n   │   └─ Check Model\n   └─ (single execution, all periods handled)\n```\nAdvantages: Self-contained, reliable, simple, maintainable, fast\n\n---\n\n## Next Steps\n\nIn the next notebook, you'll:\n1. Learn how to **run jobs** using `client.create_job_run()`\n2. Monitor job execution and track progress\n3. Handle errors and job failures\n4. Implement the complete automated monitoring pipeline"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}